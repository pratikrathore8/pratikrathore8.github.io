<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Pratik Rathore</title>
  
  <meta name="author" content="Pratik Rathore">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Pratik Rathore</name>
              </p>
              <p>
                I am a third-year PhD student in the Electrical Engineering department at <a href="https://ee.stanford.edu">Stanford University</a>, interested in optimization for machine learning, advised by <a href="https://web.stanford.edu/~udell/">Madeleine Udell</a>.
              </p>
              <p>
                Before Stanford, I graduated from the <a href="https://www.umd.edu">University of Maryland</a> with a double degree in Electrical Engineering and Mathematics. As an undergraduate, I completed internships at <a href="https://str.us">STR</a>, where I conducted research on radar image processing,
                and <a href="https://www.lockheedmartin.com">Lockheed Martin</a>, where I reviewed and tested computational models for satellites. 
                I also conducted research in number theory, for which I received the <a href = "https://www-math.umd.edu/undergraduate/math-majors.html?id=166">Dan Shanks Award</a> from the University of Maryland Math Department. 
              </p>
              <p style="text-align:center">
                <a href="mailto:pratikr@stanford.edu">Email</a> &nbsp/&nbsp
                <a href="data/PratikRathore-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=9WOwy2sAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/pratikrathore/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/pratikrathore8/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/PratikRathore_v2.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/PratikRathore_circle_v2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in using randomization and preconditioning to design fast, scalable optimization algorithms for machine learning. Recently, I've been thinking about challenges in training physics-informed neural networks and using randomized low-rank approximations to develop new algorithms for convex, finite-sum optimization and deep learning.
              </p>
              <p>
                <sup>*</sup> denotes equal contribution.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2402.01868">
                <papertitle>Challenges in Training PINNs: A Loss Landscape Perspective</papertitle>
              </a>
              <br>
              <strong>Pratik Rathore</strong>,
              Weimu Lei,
              Zachary Frangella,
              <a href="https://lugroup.yale.edu/">Lu Lu</a>,
              <a href="https://web.stanford.edu/~udell/">Madeleine Udell</a>
              <br>
              <em>in submission</em>
              <br>
              <a href="https://arxiv.org/abs/2402.0186">arXiv</a>
              <p></p>
              <p>
                We study challenges in training physics-informed neural networks. We link training issues to ill-conditioning of the loss,
                and show a combined Adam and L-BFGS approach, along with a new optimizer, NysNewton-CG, enhances PINN performance.
              </p>
            </td>
          </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2309.02014">
                <papertitle>PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates</papertitle>
              </a>
              <br>
              Zachary Frangella<sup>*</sup>,
              <strong>Pratik Rathore<sup>*</sup></strong>,
              Shipu Zhao,
              <a href="https://web.stanford.edu/~udell/">Madeleine Udell</a>
              <br>
              <em>in submission</em>
              <br>
              <a href="https://arxiv.org/abs/2309.02014">arXiv</a>
              /
              <a href="https://github.com/udellgroup/PROMISE">code</a>
              <p></p>
              <p>
                We propose PROMISE, a family of preconditioned stochastic optimization methods that use scalable, randomized curvature estimates to solve
                large-scale, ill-conditioned convex optimization problems in machine learning.
                PROMISE methods, with <em>default</em> hyperparameters, outperform popular <em>tuned</em> stochastic optimizers on ridge and logistic regression.
                Furthermore, we introduce <em>quadratic regularity</em>, which determines the speed of linear convergence for PROMISE methods
                and allows us to obtain improved rates for ridge regresison.
              </p>
            </td>
          </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2211.08597">
                <papertitle>SketchySGD: Reliable Stochastic Optimization via Randomized Curvature Estimates</papertitle>
              </a>
              <br>
              Zachary Frangella,
              <strong>Pratik Rathore</strong>,
              Shipu Zhao,
              <a href="https://web.stanford.edu/~udell/">Madeleine Udell</a>
              <br>
              <em>in submission</em>
              <br>
              <a href="https://arxiv.org/abs/2211.08597">arXiv</a>
              /
              <a href="https://github.com/udellgroup/SketchySGD/tree/main/convex">code</a>
              <p></p>
              <p>
                We use techniques from randomized numerical linear algebra to develop a fast, scalable, preconditioned stochastic gradient method for convex machine learning problems.
              </p>
            </td>
          </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1808.10027">
                <papertitle>There are no Cube-free Descartes Numbers with Exactly Seven Distinct Prime Factors</papertitle>
              </a>
              <br>
              <strong>Pratik Rathore</strong>
              <br>
              <em>preprint</em>
              <br>
              <a href="https://arxiv.org/abs/1808.10027">arXiv</a>
              <p></p>
              <p>
              We prove new results regarding the prime factorizations of Descartes numbers, a family of odd spoof perfect numbers.
              </p>
            </td>
        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/stanford_logo.png" alt="stanford" width="150" height="150">
            </td>
            <td width="75%" valign="center">
              CA, Optimization (CME 307), Winter 2024<br>
              CA, Convex Optimization II (EE 364B), Spring 2023
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/umd.png" alt="umd" width="150" height="150">
            </td>
            <td width="75%" valign="center">
              TA, Intermediate Programming Concepts for Engineers (ENEE 150), Spring 2021
            </td>
          </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template borrowed from <a href="https://jonbarron.info">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

  <!-- <a title="Privacy-friendly Web Analytics" href="https://clicky.com/101299767"><img alt="Clicky"
      src="//static.getclicky.com/media/links/badge.gif" border="0" /></a> -->
  <script async src="//static.getclicky.com/101299767.js"></script>
  <!-- <noscript>
    <p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101299767ns.gif" /></p>
  </noscript> -->
</body>

</html>
